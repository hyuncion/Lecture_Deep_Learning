# -*- coding: utf-8 -*-
"""LSTM_SGD_100d.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U5Lh8rsMu38-a7jDEZoGIza7_bOrx_rd
"""

import csv
import numpy as np
import emoji
import pandas as pd
import math
import matplotlib.pyplot as plt

def read_glove_vecs(glove_file):
    with open(glove_file, 'r', encoding="utf8") as f:
        words = set()
        word_to_vec_map = {}
        for line in f:
            line = line.strip().split()
            curr_word = line[0]
            words.add(curr_word)
            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)

        i = 1
        words_to_index = {}
        index_to_words = {}
        for w in sorted(words):
            words_to_index[w] = i
            index_to_words[i] = w
            i = i + 1
    return words_to_index, index_to_words, word_to_vec_map


def read_csv(filename = 'emojify_data.csv'):
    phrase = []
    emoji = []

    with open (filename) as csvDataFile:
        csvReader = csv.reader(csvDataFile)

        for row in csvReader:
            phrase.append(row[0])
            emoji.append(row[1])

    X = np.asarray(phrase)
    Y = np.asarray(emoji, dtype=int)

    return X, Y


emoji_dictionary = {"0": "\u2764\uFE0F",    # :heart: prints a black instead of red heart depending on the font
                    "1": ":baseball:",
                    "2": ":smile:",
                    "3": ":disappointed:",
                    "4": ":fork_and_knife:"}

def label_to_emoji(label):
    """
    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed
    """
    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)


def print_predictions(X, pred):
    print()
    for i in range(X.shape[0]):
        print(X[i], label_to_emoji(int(pred[i])))

# Word Embedding
def word_embedding(sentence, w2v):
  words = sentence.split()
  vectors = []
  for o in words:
    o = o.lower()
    o = w2v[o]
    vectors.append(o)
  return np.array(vectors)

def one_hot_encoding(number, num_classes):
    one_hot = np.zeros(num_classes)
    one_hot[number] = 1
    return one_hot

# Data Load
w2i50, i2w50, w2v50 = read_glove_vecs('glove.6B.50d.txt')
w2i100, i2w100, w2v100 = read_glove_vecs('glove.6B.100d.txt')
train_x, train_y = read_csv('train_emoji.csv')
test_x, test_y = read_csv('test_emoji.csv')

# Activation Layer
class Activation_Layer():
  def __init__(self, typ):
    self.typ = typ
    types = ['softmax'] # tanh, sigmoid는 따로 구현.
    if not self.typ in types:
      raise ValueError

  def forward(self, x):
    if self.typ == 'softmax':
      c = np.max(x)
      exp_x = np.exp(x-c)
      sum_exp_x = np.sum(exp_x)
      y = exp_x / sum_exp_x
      return y

  def backward(self, x, y = None):
    if self.typ == 'softmax':
      return x-y

# Linear Layer
class Linear_Layer():
  def __init__(self, input_dim=20, output_dim=5):
    self.input_dim = input_dim
    self.output_dim = output_dim
    xavier_scale = np.sqrt(1/(self.input_dim))
    self.Wy = np.random.uniform(-1*xavier_scale, xavier_scale, (self.input_dim, self.output_dim)) # (20,5)
    self.b = np.zeros((1,self.output_dim)) # (1,5)
    self.dWy = None # (20,5)
    self.db = None # (1,5)
    self.x = None # (1,20)
    self.pred_y = None # (1,5)
    self.softmax = Activation_Layer('softmax')

  def forward(self, x):
    self.x = x
    y = np.dot(x, self.Wy) + self.b
    pred_y = self.softmax.forward(y)
    self.pred_y = pred_y
    return pred_y

  def backward(self, y):
    err = self.softmax.backward(self.pred_y, y)
    self.dWy = np.dot(self.x.T, err)
    self.db = err
    temp = np.dot(self.Wy, err.T) # (20,5) dot (5,1) = (20,1)
    return temp.T # (1,20)

# LSTM Layer
class LSTM_Layer():
  def __init__(self, word_dims=50, hidden_dims=16):
    self.hidden_dims = hidden_dims
    self.word_dims = word_dims
    xavier_scale1 = np.sqrt(1.0/(self.word_dims))
    xavier_scale2 = np.sqrt(1.0/(self.hidden_dims))

    self.pred_y = None
    self.Wx = np.random.uniform(-1 * xavier_scale1, xavier_scale1, (self.word_dims, self.hidden_dims*4)) # (50,10)
    self.Wh = np.random.uniform(-1 * xavier_scale2, xavier_scale2, (self.hidden_dims, self.hidden_dims*4)) # (10,10)
    self.b = np.zeros((1, self.hidden_dims*4)) # (1,10)

    self.dWx = np.zeros_like(self.Wx) # (50,10)
    self.dWh = np.zeros_like(self.Wh) # (10,10)
    self.db = np.zeros_like(self.b) # (1,10)

    self.cache_x = []
    self.cache_prev_c = []
    self.cache_next_c = []
    self.cache_h = []
    self.cache_f = []
    self.cache_i = []
    self.cache_o = []
    self.cache_g = []


  def sigmoid(self,x):
    return 1/(1+np.exp(-x))


  def forward(self, x, prev_h=None, prev_c=None): # x == word (1,50)
    if prev_h is None:
      prev_h = np.zeros((1,self.hidden_dims)) # (1,10)
    if prev_c is None:
      prev_c = np.zeros((1,self.hidden_dims))

    a = np.dot(x, self.Wx) + np.dot(prev_h, self.Wh) + self.b
    # f, g, i, o
    f = self.sigmoid(a[:, :self.hidden_dims])
    g = np.tanh(a[:, self.hidden_dims:2*self.hidden_dims])
    i = self.sigmoid(a[:, 2*self.hidden_dims:3*self.hidden_dims])
    o = self.sigmoid(a[:, 3*self.hidden_dims:])
    next_c = f*prev_c + g*i
    next_h = o * np.tanh(next_c)

    self.cache_x.append(x)
    self.cache_prev_c.append(prev_c)
    self.cache_next_c.append(next_c)
    self.cache_h.append(prev_h)
    self.cache_f.append(f)
    self.cache_i.append(i)
    self.cache_o.append(o)
    self.cache_g.append(g)
    return next_h, next_c

  def backward(self, prev_grads, num): # prev_grads : list
    dx_list = []
    n = num
    for prev_grad in prev_grads: # up to down
      for o in reversed(range(n)): # right to left
        # df, dg, di, do
        df = prev_grad * self.cache_o[o] * (1-self.cache_next_c[o]**2) * self.cache_prev_c[o] * self.cache_f[o] * (1-self.cache_f[o])
        dg = prev_grad * self.cache_o[o] * (1-self.cache_next_c[o]**2) * self.cache_i[o] * (1-self.cache_g[o]**2)
        di = prev_grad * self.cache_o[o] * (1-self.cache_next_c[o]**2) * self.cache_g[o] * self.cache_i[o] * (1-self.cache_i[o])
        do = prev_grad * np.tanh(self.cache_next_c[o]) * self.cache_o[o] * (1-self.cache_o[o])
        da = np.hstack((df, dg, di, do))
        self.dWh += np.dot(self.cache_h[o].T, da)
        self.dWx += np.dot(self.cache_x[o].T, da)
        self.db += da
        pre_grad = np.dot(da, self.Wh.T) # dh
        dx = np.dot(da, self.Wx.T)
        dx_list.append(dx)
      n -= 1
    return dx_list

# LSTM Model
class LSTM_Model():
  def __init__(self, learning_rate = 0.001, optimizer = 'SGD',beta1 = 0.9, beta2 = 0.999, dp_ratio = 0.0, d100 = False):
    if d100 == False:
      self.layers = [LSTM_Layer(50,128), LSTM_Layer(128,128), Linear_Layer(128, 5)]
    elif d100 == True:
      self.layers = [LSTM_Layer(100,128), LSTM_Layer(128,128), Linear_Layer(128, 5)]

    self.prev_layer_h = []
    self.num_words = 0
    self.learning_rate = learning_rate
    self.pred_y = None
    self.y = None

    self.loss = []
    self.total = 0
    self.yes = 0
    self.emojis = []

    # Dropout
    self.dp_ratio = dp_ratio
    self.mask1 = None
    self.mask2 = None

    # SGD or ADAM
    self.optimizer = optimizer
    if self.optimizer == 'ADAM': # for ADAM
      self.beta1 = beta1
      self.beta2 = beta2
      self.t = 0
      self.m1 = 0
      self.m2 = 0
      self.m3 = 0
      self.m4 = 0
      self.m5 = 0
      self.m6 = 0
      self.m7 = 0
      self.m8 = 0
      self.v1 = 0
      self.v2 = 0
      self.v3 = 0
      self.v4 = 0
      self.v5 = 0
      self.v6 = 0
      self.v7 = 0
      self.v8 = 0

  def forward(self, x, y): # x = sentence (5,50)
    self.num_words = x.shape[0]
    prev_h = None  # layer 1
    prev_c = None  # layer 1
    for word in x:
      word = word.reshape(1,-1)
      prev_h, prev_c = self.layers[0].forward(word, prev_h, prev_c)
      # dropout forward!
      if self.dp:
        temp1 = prev_h.copy()
        temp1[self.mask1] = 0
        self.prev_layer_h.append(temp1)
      else:
        self.prev_layer_h.append(prev_h)

    prev_c = None # layer2
    prev_h = None # layer2
    for i in self.prev_layer_h:
      prev_h, prev_c = self.layers[1].forward(i, prev_h, prev_c)

    # dropout forward!
    if self.dp:
      temp2 = prev_h.copy()
      temp2[self.mask2] = 0

    else:
      temp2 = prev_h

    # linear layer
    pred_y = self.layers[2].forward(temp2)
    self.pred_y = pred_y
    self.y = y.reshape(1,-1)
    return pred_y


  def backward(self): # y = (1,5) processing
    y = self.y
    dL_dh2 = [self.layers[2].backward(y)] # (1,10)
    if self.dp:
      temp1 = dL_dh2.copy()
      dL_dh2 = []
      for i in temp1:
        i[self.mask2]=0
        dL_dh2.append(i)

    dL_dh1 = self.layers[1].backward(dL_dh2, self.num_words)
    if self.dp:
      temp2 = dL_dh1.copy()
      dL_dh1 = []
      for i in temp2:
        i[self.mask1]=0
        dL_dh1.append(i)

    dL_dh0 = self.layers[0].backward(dL_dh1, self.num_words) # dL_dh1 : (1,50)이 여럿 들어간 list


  def update(self):
    if self.optimizer == 'SGD':
      self.layers[2].Wy -= self.learning_rate * self.layers[2].dWy
      self.layers[2].b -= self.learning_rate * self.layers[2].db
      self.layers[1].Wh -= self.learning_rate * self.layers[1].dWh
      self.layers[1].Wx -= self.learning_rate * self.layers[1].dWx
      self.layers[1].b -= self.learning_rate * self.layers[1].db
      self.layers[0].Wh -= self.learning_rate * self.layers[0].dWh
      self.layers[0].Wx -= self.learning_rate * self.layers[0].dWx
      self.layers[0].b -= self.learning_rate *  self.layers[0].db
    elif self.optimizer == 'ADAM':
      # implement
      epsilon = 1e-8
      self.t += 1

      self.m1 = self.beta1*self.m1 + (1-self.beta1)*self.layers[2].dWy
      self.v1 = self.beta2*self.v1 + (1-self.beta2)*(self.layers[2].dWy**2)
      m1_hat = self.m1 / (1-self.beta1**self.t)
      v1_hat = self.v1 / (1-self.beta2**self.t)
      self.layers[2].Wy -= self.learning_rate*(m1_hat/(np.sqrt(v1_hat)+epsilon))

      self.m2 = self.beta1*self.m2 + (1-self.beta1)*self.layers[2].db
      self.v2 = self.beta2*self.v2 + (1-self.beta2)*(self.layers[2].db**2)
      m2_hat = self.m2 / (1-self.beta1**self.t)
      v2_hat = self.v2 / (1-self.beta2**self.t)
      self.layers[2].b -= self.learning_rate*(m2_hat/(np.sqrt(v2_hat)+epsilon))

      self.m3 = self.beta1*self.m3 + (1-self.beta1)*self.layers[1].dWh
      self.v3 = self.beta2*self.v3 + (1-self.beta2)*(self.layers[1].dWh**2)
      m3_hat = self.m3 / (1-self.beta1**self.t)
      v3_hat = self.v3 / (1-self.beta2**self.t)
      self.layers[1].Wh -= self.learning_rate*(m3_hat/(np.sqrt(v3_hat)+epsilon))

      self.m4 = self.beta1*self.m4 + (1-self.beta1)*self.layers[1].dWx
      self.v4 = self.beta2*self.v4 + (1-self.beta2)*(self.layers[1].dWx**2)
      m4_hat = self.m4 / (1-self.beta1**self.t)
      v4_hat = self.v4 / (1-self.beta2**self.t)
      self.layers[1].Wx -= self.learning_rate*(m4_hat/(np.sqrt(v4_hat)+epsilon))

      self.m5 = self.beta1*self.m5 + (1-self.beta1)*self.layers[1].db
      self.v5 = self.beta2*self.v5 + (1-self.beta2)*(self.layers[1].db**2)
      m5_hat = self.m5 / (1-self.beta1**self.t)
      v5_hat = self.v5 / (1-self.beta2**self.t)
      self.layers[1].b -= self.learning_rate*(m5_hat/(np.sqrt(v5_hat)+epsilon))

      self.m6 = self.beta1*self.m6 + (1-self.beta1)*self.layers[0].dWh
      self.v6 = self.beta2*self.v6 + (1-self.beta2)*(self.layers[0].dWh**2)
      m6_hat = self.m6 / (1-self.beta1**self.t)
      v6_hat = self.v6 / (1-self.beta2**self.t)
      self.layers[0].Wh -= self.learning_rate*(m6_hat/(np.sqrt(v6_hat)+epsilon))

      self.m7 = self.beta1*self.m7 + (1-self.beta1)*self.layers[0].dWx
      self.v7 = self.beta2*self.v7 + (1-self.beta2)*(self.layers[0].dWx**2)
      m7_hat = self.m7 / (1-self.beta1**self.t)
      v7_hat = self.v7 / (1-self.beta2**self.t)
      self.layers[0].Wx -= self.learning_rate*(m7_hat/(np.sqrt(v7_hat)+epsilon))

      self.m8 = self.beta1*self.m8 + (1-self.beta1)*self.layers[0].db
      self.v8 = self.beta2*self.v8 + (1-self.beta2)*(self.layers[0].db**2)
      m8_hat = self.m8 / (1-self.beta1**self.t)
      v8_hat = self.v8 / (1-self.beta2**self.t)
      self.layers[0].b -= self.learning_rate*(m8_hat/(np.sqrt(v8_hat)+epsilon))


  def reset(self):
    self.prev_layer_h = []
    self.layers[2].dWy = 0
    self.layers[2].db = 0
    self.layers[1].dWh = 0
    self.layers[1].dWx = 0
    self.layers[1].db = 0
    self.layers[0].dWh = 0
    self.layers[0].dWx = 0
    self.layers[0].db = 0
    self.layers[1].cache_h = []
    self.layers[1].cache_x = []
    self.layers[1].cache_prev_c = []
    self.layers[1].cache_next_c = []
    self.layers[1].cache_f = []
    self.layers[1].cache_i = []
    self.layers[1].cache_o = []
    self.layers[1].cache_g = []
    self.layers[0].cache_h = []
    self.layers[0].cache_x = []
    self.layers[0].cache_prev_c = []
    self.layers[0].cache_next_c = []
    self.layers[0].cache_f = []
    self.layers[0].cache_i = []
    self.layers[0].cache_o = []
    self.layers[0].cache_g = []

  def loss_function(self):
    e = 1e-15
    pred_y = np.squeeze(self.pred_y)
    pred_y = np.clip(pred_y, e, 1-e)
    loss = 0
    for i in range(len(pred_y)):
      loss += -1 * self.y[0][i] * np.log(pred_y[i] + e)
    self.loss.append(loss)
    return loss

  def train(self, x, y):
    # Dropout
    if self.dp_ratio > 0.0:
      self.dp = True
      self.mask1 = (np.random.rand(1,128) < self.dp_ratio)
      self.mask2 = (np.random.rand(1,128) < self.dp_ratio)
    else:
      self.dp = False

    self.forward(x, y)
    self.loss_function()
    self.backward()
    self.update()
    self.reset()

  def get_accuracy(self):
    return self.yes / self.total

  def get_emojis(self):
    return self.emojis

  def test(self, x, y):
    self.dp = False
    self.forward(x,y)
    self.loss_function()
    self.total += 1
    if np.argmax(self.pred_y) == np.argmax(self.y):
      self.yes += 1
    self.emojis.append(np.argmax(self.pred_y))
    self.reset()

# Implement 4 (vanilla LSTM with 100d)
train_loss4 = []
test_loss4 = []
my_LSTM4 = LSTM_Model(learning_rate=0.1, d100 = True)
for e in range(30):
  for i in range(len(train_x)):
    rand = np.random.randint(len(train_x))
    x = word_embedding(train_x[rand], w2v100)
    y = one_hot_encoding(train_y[rand],5)
    my_LSTM4.train(x,y)
  train_loss4.append(sum(my_LSTM4.loss)/len(my_LSTM4.loss))
  my_LSTM4.loss = []
  for i in range(len(test_y)):
    x = word_embedding(test_x[i], w2v100)
    y = one_hot_encoding(test_y[i],5)
    my_LSTM4.test(x,y)
  test_loss4.append(sum(my_LSTM4.loss)/len(my_LSTM4.loss))
  my_LSTM4.loss = []

plt.plot(train_loss4, label='Train loss', color='blue')
plt.plot(test_loss4, label='Test Loss', color='red')
plt.title('Training & Test Loss Over Epochs with LSTM4')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

my_LSTM4.total = 0
my_LSTM4.yes = 0
my_LSTM4.emojis = []
for i in range(len(test_x)):
  x = word_embedding(test_x[i], w2v100)
  y = one_hot_encoding(test_y[i],5)
  my_LSTM4.test(x,y)
acc4 = my_LSTM4.get_accuracy()
emojis4 = my_LSTM4.get_emojis()
print(f"Accuracy : {acc4}")
print(f"Emojis : {emojis4}")