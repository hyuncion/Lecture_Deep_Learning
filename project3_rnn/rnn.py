# -*- coding: utf-8 -*-
"""RNN final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UdmkGBEDIKgtZWu_qfYIaFVb80CtPQFs
"""

import csv
import numpy as np
import emoji
import pandas as pd
import math
import matplotlib.pyplot as plt

def read_glove_vecs(glove_file):
    with open(glove_file, 'r', encoding="utf8") as f:
        words = set()
        word_to_vec_map = {}
        for line in f:
            line = line.strip().split()
            curr_word = line[0]
            words.add(curr_word)
            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)

        i = 1
        words_to_index = {}
        index_to_words = {}
        for w in sorted(words):
            words_to_index[w] = i
            index_to_words[i] = w
            i = i + 1
    return words_to_index, index_to_words, word_to_vec_map


def read_csv(filename = 'emojify_data.csv'):
    phrase = []
    emoji = []

    with open (filename) as csvDataFile:
        csvReader = csv.reader(csvDataFile)

        for row in csvReader:
            phrase.append(row[0])
            emoji.append(row[1])

    X = np.asarray(phrase)
    Y = np.asarray(emoji, dtype=int)

    return X, Y


emoji_dictionary = {"0": "\u2764\uFE0F",    # :heart: prints a black instead of red heart depending on the font
                    "1": ":baseball:",
                    "2": ":smile:",
                    "3": ":disappointed:",
                    "4": ":fork_and_knife:"}

def label_to_emoji(label):
    """
    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed
    """
    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)


def print_predictions(X, pred):
    print()
    for i in range(X.shape[0]):
        print(X[i], label_to_emoji(int(pred[i])))

# Word Embedding
def word_embedding(sentence, w2v):
  words = sentence.split()
  vectors = []
  for o in words:
    o = o.lower()
    o = w2v[o]
    vectors.append(o)
  return np.array(vectors)

def one_hot_encoding(number, num_classes):
    one_hot = np.zeros(num_classes)
    one_hot[number] = 1
    return one_hot

# Data Load
w2i50, i2w50, w2v50 = read_glove_vecs('glove.6B.50d.txt')
train_x, train_y = read_csv('train_emoji.csv')
test_x, test_y = read_csv('test_emoji.csv')

# Activation Layer
class Activation_Layer():
  def __init__(self, typ):
    self.typ = typ
    types = ['softmax'] # tanh는 따로 구현
    if not self.typ in types:
      raise ValueError

  def forward(self, x):
    if self.typ == 'softmax':
      c = np.max(x)
      exp_x = np.exp(x-c)
      sum_exp_x = np.sum(exp_x)
      y = exp_x / sum_exp_x
      return y


  def backward(self, x, y = None):
    if self.typ == 'softmax':
      return x-y

# Linear Layer
class Linear_Layer():
  def __init__(self, input_dim=20, output_dim=5):
    self.input_dim = input_dim
    self.output_dim = output_dim
    xavier_scale = np.sqrt(1/(self.input_dim))
    self.Wy = np.random.uniform(-1*xavier_scale, xavier_scale, (self.input_dim, self.output_dim)) # (20,5)
    self.b = np.zeros((1,self.output_dim)) # (1,5)
    self.dWy = None # (20,5)
    self.db = None # (1,5)
    self.x = None # (1,20)
    self.pred_y = None # (1,5)
    self.softmax = Activation_Layer('softmax')

  def forward(self, x):
    self.x = x
    y = np.dot(x, self.Wy) + self.b
    pred_y = self.softmax.forward(y)
    self.pred_y = pred_y
    return pred_y

  def backward(self, y):
    err = self.softmax.backward(self.pred_y, y)
    self.dWy = np.dot(self.x.T, err)
    self.db = err
    temp = np.dot(self.Wy, err.T) # (20,5) dot (5,1) = (20,1)
    return temp.T # (1,20)

# RNN Layer
class RNN_Layer():
  def __init__(self, word_dims=50, hidden_dims=10):
    self.hidden_dims = hidden_dims
    self.word_dims = word_dims
    xavier_scale1 = np.sqrt(1.0/(self.word_dims))
    xavier_scale2 = np.sqrt(1.0/(self.hidden_dims))

    self.pred_y = None
    self.Wx = np.random.uniform(-1 * xavier_scale1, xavier_scale1, (self.word_dims, self.hidden_dims)) # (50,10)
    self.Wh = np.random.uniform(-1 * xavier_scale2, xavier_scale2, (self.hidden_dims, self.hidden_dims)) # (10,10)
    self.b = np.zeros((1, self.hidden_dims)) # (1,10)

    self.dWx = np.zeros_like(self.Wx) # (50,10)
    self.dWh = np.zeros_like(self.Wh) # (10,10)
    self.db = np.zeros_like(self.b) # (1,10)

    self.cache_x = [] # input
    self.cache_a = [] # before tanh
    self.cache_h = [] # after tanh


  def forward(self, x, prev_h=None): # x == word (1,50)
    if prev_h is None:
      prev_h = np.zeros((1, self.hidden_dims)) # (1,10)
    a = np.dot(x, self.Wx) + np.dot(prev_h, self.Wh) + self.b
    next_h = np.tanh(a)
    self.cache_x.append(x)
    self.cache_h.append(prev_h)
    self.cache_a.append(a)
    return next_h

  def backward(self, prev_grads, num): # prev_grads : list
    dx_list = []
    n = num
    for prev_grad in prev_grads:
      for o in reversed(range(n)):
        da = prev_grad * (1-(np.tanh(self.cache_a[o])**2)) # (1,20)
        self.db += da # (1,20)
        self.dWh += np.dot(self.cache_h[o].T, da) # (20,20)
        self.dWx += np.dot(self.cache_x[o].T, da) # (100,20)
        prev_grad = np.dot(da, self.Wh.T) # (1,20)
        dx = np.dot(da, self.Wx.T) # (1,100)
        dx_list.append(dx)
      n -= 1
    return dx_list

# RNN Model
class RNN_Model():
  def __init__(self, learning_rate = 0.001):
    self.layers = [RNN_Layer(50,128), RNN_Layer(128,128), Linear_Layer(128, 5)]
    self.prev_layer_h = []
    self.next_layer_da = []
    self.num_words = 0
    self.learning_rate = learning_rate
    self.pred_y = None
    self.y = None
    self.loss = []
    self.total = 0
    self.yes = 0
    self.emojis = []

  def forward(self, x, y): # x = sentence (5,50)
    self.num_words = x.shape[0]
    prev_h = None
    for word in x: # layer 1
      word = word.reshape(1,-1)
      prev_h = self.layers[0].forward(word, prev_h)
      self.prev_layer_h.append(prev_h)

    prev_h = None # layer2
    for i in self.prev_layer_h:
      prev_h = self.layers[1].forward(i, prev_h)

    # linear layer
    pred_y = self.layers[2].forward(prev_h)
    self.pred_y = pred_y
    self.y = y.reshape(1,-1)
    return pred_y


  def backward(self): # y = (1,5) processing
    y = self.y
    dL_dh2 = [self.layers[2].backward(y)] # (1,10)
    dL_dh1 = self.layers[1].backward(dL_dh2, self.num_words)
    dL_dh0 = self.layers[0].backward(dL_dh1, self.num_words) # dL_dh1 : (1,50)이 여럿 들어간 list


  def update(self):
    self.layers[2].Wy -= self.learning_rate * self.layers[2].dWy
    self.layers[2].b -= self.learning_rate * self.layers[2].db
    self.layers[1].Wh -= self.learning_rate * self.layers[1].dWh
    self.layers[1].Wx -= self.learning_rate * self.layers[1].dWx
    self.layers[1].b -= self.learning_rate * self.layers[1].db
    self.layers[0].Wh -= self.learning_rate * self.layers[0].dWh
    self.layers[0].Wx -= self.learning_rate * self.layers[0].dWx
    self.layers[0].b -= self.learning_rate *  self.layers[0].db

  def reset(self):
    self.prev_layer_h = []
    self.layers[1].dWh = 0
    self.layers[1].dWx = 0
    self.layers[1].db = 0
    self.layers[0].dWh = 0
    self.layers[0].dWx = 0
    self.layers[0].db = 0
    self.layers[1].cache_h = []
    self.layers[1].cache_x = []
    self.layers[1].cache_a = []
    self.layers[0].cache_h = []
    self.layers[0].cache_x = []
    self.layers[0].cache_a = []


  def loss_function(self):
    e = 1e-20
    pred_y = np.squeeze(self.pred_y)
    pred_y = np.clip(pred_y, e, 1-e)
    loss = 0
    for i in range(len(pred_y)):
      loss += -1 * self.y[0][i] * np.log(pred_y[i] + e)
    self.loss.append(loss)
    return loss

  def train(self, x, y):
    self.forward(x, y)
    self.loss_function()
    self.backward()
    self.update()
    self.reset()

  def get_accuracy(self):
    return self.yes / self.total

  def get_emojis(self):
    return self.emojis

  def test(self, x, y):
    self.forward(x,y)
    self.loss_function()
    self.total += 1
    if np.argmax(self.pred_y) == np.argmax(self.y):
      self.yes += 1
    self.emojis.append(np.argmax(self.pred_y))
    self.reset()

# Implement
epoch = 60
learning_rate = 0.01
train_loss = []
test_loss = []
my_RNN = RNN_Model(learning_rate)
for e in range(epoch):
  for i in range(len(train_x)):
    rand = np.random.randint(len(train_x))
    x = word_embedding(train_x[rand], w2v50)
    y = one_hot_encoding(train_y[rand],5)
    my_RNN.train(x,y)
  train_loss.append(sum(my_RNN.loss)/len(my_RNN.loss))
  my_RNN.loss = []
  for i in range(len(test_y)):
    x = word_embedding(test_x[i], w2v50)
    y = one_hot_encoding(test_y[i],5)
    my_RNN.test(x,y)
  test_loss.append(sum(my_RNN.loss)/len(my_RNN.loss))
  my_RNN.loss = []

plt.plot(train_loss, label='Train loss', color='blue')
plt.plot(test_loss, label='Test Loss', color='red')
plt.title('Training & Test Loss Over Epochs with RNN')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

my_RNN.total = 0
my_RNN.yes = 0
my_RNN.emojis = []
for i in range(len(test_x)):
  x = word_embedding(test_x[i], w2v50)
  y = one_hot_encoding(test_y[i],5)
  my_RNN.test(x,y)
acc = my_RNN.get_accuracy()
emojis = my_RNN.get_emojis()
print(f"Accuracy : {acc}")
print(f"Emojis : {emojis}")

# Accuracy comparision for each test
# Training loss graph
# All emojis for test set