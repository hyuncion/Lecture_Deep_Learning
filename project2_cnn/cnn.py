# -*- coding: utf-8 -*-
"""cnn final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ct-mH2Z6upUC4Gi8ELlJ_0UHqhgNeot
"""

import gzip
import numpy as np
from pathlib import Path
import math
import matplotlib.pyplot as plt

#### DATA LOAD ####
class Dataloader():
    def __init__(self, path='', is_train=True, shuffle=True, batch_size=8):
        path = Path(path)
        imagePath = Path(path/'train-images-idx3-ubyte.gz') if is_train else Path(path/'t10k-images-idx3-ubyte.gz')
        labelPath = Path(path/'train-labels-idx1-ubyte.gz') if is_train else Path(path/'t10k-labels-idx1-ubyte.gz')

        self.batch_size = batch_size
        self.images = self.loadImages(imagePath)
        self.labels = self.loadLabels(labelPath)
        self.index = 0
        self.idx = np.arange(0, self.images.shape[0])
        if shuffle: np.random.shuffle(self.idx) # shuffle images

    def __len__(self):
        n_images, _, _, _ = self.images.shape
        n_images = math.ceil(n_images / self.batch_size)
        return n_images

    def __iter__(self):
        return datasetIterator(self)

    def __getitem__(self, index):
        image = self.images[self.idx[index * self.batch_size:(index + 1) * self.batch_size]]
        label = self.labels[self.idx[index * self.batch_size:(index + 1) * self.batch_size]]
        image = image/255.0
        return image, label

    def loadImages(self, path):
        with gzip.open(path) as f:
            images = np.frombuffer(f.read(), 'B', offset=16)
            images = images.reshape(-1, 1, 28, 28).astype(np.float32)
            return images

    def loadLabels(self, path):
        with gzip.open(path) as f:
            labels = np.frombuffer(f.read(), 'B', offset=8)
            rows = len(labels)
            cols = labels.max() + 1
            one_hot = np.zeros((rows, cols)).astype(np.uint8)
            one_hot[np.arange(rows), labels] = 1
            one_hot = one_hot.astype(np.float64)
            return one_hot

# for enumerate magic python function returns Iterator
class datasetIterator():
    def __init__(self, dataloader):
        self.index = 0
        self.dataloader = dataloader

    def __next__(self):
        if self.index < len(self.dataloader):
            item = self.dataloader[self.index]
            self.index += 1
            return item
        # end of iteration
        raise StopIteration

def im2col(input_data, fh, fw, stride=1, pad=0):
    b, c, h, w = input_data.shape
    oh = (h + 2*pad - fh) // stride + 1
    ow = (w + 2*pad - fw) // stride + 1
    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')
    col = np.zeros((b, c, fh, fw, oh, ow))
    for y in range(fh):
        y_max = y + stride*oh
        for x in range(fw):
            x_max = x + stride*ow
            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]
    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(b*oh*ow, -1)
    return col

def col2im(col, input_shape, fh, fw, stride=1, pad=0):
    b, c, h, w = input_shape
    oh = (h + 2*pad - fh) // stride + 1
    ow = (w + 2*pad - fw) // stride + 1
    col = col.reshape(b, oh, ow, c, fh, fw).transpose(0, 3, 4, 5, 1, 2)
    img = np.zeros((b, c, h + 2*pad + stride - 1, w + 2*pad + stride - 1))
    for y in range(fh):
        y_max = y + stride*oh
        for x in range(fw):
            x_max = x + stride*ow
            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]
    return img[:, :, pad:h + pad, pad:w + pad]

# Base (relu, softmax)
class Activation():
  def __init__(self, typ):
    self.typ = typ
    self.inp = None
    types = ['relu', 'linear', 'softmax']
    if not self.typ in types:
      raise ValueError

  def forward(self, x):
    if self.typ == 'relu':
      self.inp = x
      return np.maximum(0, x)

    elif self.typ == 'linear':
      return x

    elif self.typ == 'softmax':
      c = np.max(x)
      exp_x = np.exp(x-c)
      sum_exp_x = np.sum(exp_x)
      y = exp_x / sum_exp_x
      return y

  def backward(self, x, y = None):
    o = np.copy(self.inp)
    if self.typ == 'relu':
      # x : backward input, y : forward input
      # y를 거른 이후, x와 multiply을 진행하자.
      o[self.inp>0] = 1
      o[self.inp<0] = 0
      o[self.inp==0] = 0
      return np.multiply(x,o)

    elif self.typ == 'linear': # array shape가 맞는지 확인해야 한다.
      return 1

    elif self.typ == 'softmax':
      return x-y

# Define the Convolution Layer class
class ConvolutionLayer():
    def __init__(self, input_channels, num_filters, filter_size, stride, padding):
        self.num_filters = num_filters
        self.filter_size = filter_size
        self.stride = stride
        self.padding = padding
        xavier_scale = np.sqrt(2.0 / (input_channels + num_filters))
        self.weights = np.random.uniform(-xavier_scale, xavier_scale, size=(num_filters, input_channels, filter_size, filter_size))
        self.grad_weights = 0
        self.col = None
        self.col_W = None
        self.last_input = None

    def forward(self, x):
        oh = (x.shape[2] + 2*self.padding - self.weights.shape[2]) // self.stride + 1
        ow = (x.shape[3] + 2*self.padding - self.weights.shape[3]) // self.stride + 1
        c = im2col(x, self.weights.shape[2], self.weights.shape[3], self.stride, self.padding)
        cw = self.weights.reshape(self.weights.shape[0], -1).T
        self.output = np.dot(c, cw)
        self.output = self.output.reshape(x.shape[0], oh, ow, -1)
        self.output = self.output.transpose(0, 3, 1, 2)
        self.col = c
        self.col_W = cw
        self.last_input = x
        return self.output

    def backward(self, d):
        d = d.transpose(0,2,3,1).reshape(-1, self.weights.shape[0])
        self.grad_weights = np.dot(self.col.T, d)
        self.grad_weights = self.grad_weights.transpose(1,0)
        self.grad_weights = self.grad_weights.reshape(self.weights.shape[0], self.weights.shape[1], self.weights.shape[2], self.weights.shape[3])
        dc = np.dot(d, self.col_W.T)
        dLdX = col2im(dc, self.last_input.shape, self.weights.shape[2], self.weights.shape[3], self.stride, self.padding)
        return dLdX

# Define the MaxPooling Layer class
class MaxPoolingLayer():
    def __init__(self, pool_size, padding=0):
        self.pool = pool_size
        self.stride = pool_size
        self.padding = padding
        self.last_input = None
        self.n = None

    def forward(self, x):
        self.last_input = x
        oh = (x.shape[2] + 2*self.padding-self.pool) // self.stride + 1
        ow = (x.shape[3] + 2*self.padding-self.pool) // self.stride + 1
        col = im2col(x, self.pool, self.pool, self.stride, self.padding)
        col = col.reshape(-1, self.pool**2)
        self.n = np.argmax(col, axis=1)
        self.output = np.max(col, axis=1).reshape(x.shape[0], oh, ow, x.shape[1]).transpose(0, 3, 1, 2)
        return self.output

    def backward(self, d):
        d = d.transpose(0, 2, 3, 1)
        dmax = np.zeros((d.size, self.pool**2))
        dmax[np.arange(self.n.size), self.n.flatten()] = d.flatten()
        dmax = dmax.reshape(d.shape + (self.pool**2,))
        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)
        dLdX = col2im(dcol, self.last_input.shape, self.pool, self.pool, self.stride, self.padding)
        return dLdX

# Define the Flatten Layer class
class FlattenLayer():
    def __init__(self):
      self.input_shape = ()

    def forward(self, x):
        # Implement the forward pass to flatten the input
        self.input_shape = x.shape  # Store the input shape for later use
        return x.reshape(x.shape[0], -1)
    def backward(self, da):
      return da.reshape(self.input_shape)

# Define the Fully Connected Layer class (similar to the one in your provided code)
class FullyConnectedLayer():
    def __init__(self, input_size, output_size):
        self.input_size = input_size
        xavier_scale = np.sqrt(2.0 / (input_size + output_size))
        self.weights = np.random.uniform(-xavier_scale, xavier_scale, size=(output_size, input_size))
        self.biases = np.zeros((output_size, 1))
        self.input = None
        self.grad_weights = 0
        self.grad_biases = 0


    def forward(self, x):
        self.input = x
        return np.dot(self.weights, x.T) + self.biases

    def backward(self, d):
        d_input = np.dot(d, self.weights)
        d_weights = np.dot(d.T, self.input)
        d_biases = np.sum(d, axis=1, keepdims=True)
        self.grad_weights = d_weights / self.input_size
        self.grad_biases = d_biases / self.input_size
        return d_input, d_weights, d_biases

# Define the CNNModel class, which assembles all the layers
class CNNModel():
    def __init__(self):
        # Create and configure the layers (Convolution, Activation, MaxPooling, etc.)
        self.conv_layer1 = ConvolutionLayer(input_channels=1, num_filters=1, filter_size=3, stride=1, padding=1)
        self.activation1 = Activation('relu')
        self.maxpool1 = MaxPoolingLayer(pool_size=2)
        self.conv_layer2 = ConvolutionLayer(input_channels=1, num_filters=1, filter_size=3, stride=1, padding=1)
        self.activation2 = Activation('relu')
        self.maxpool2 = MaxPoolingLayer(pool_size=2)
        self.flatten = FlattenLayer()
        self.fc_layer = FullyConnectedLayer(input_size= 7 * 7 * 1, output_size=10)
        self.softmax = Activation('softmax')

        # appendix
        self.Loss_train = []
        self.Loss_test = []
        self.la = -1
        self.confusion_true = []
        self.confusion_prediction = []
        self.top3 = [[],[],[],[],[],[],[],[],[],[]]

    def reset(self):
        self.Loss = 0
        self.la = -1
        self.confusion_true = []
        self.confusion_prediction = []
        self.top3 = [[],[],[],[],[],[],[],[],[],[]]


    def forward(self, x):
        # Implement the forward pass for the entire network
        x = self.conv_layer1.forward(x)
        x = self.activation1.forward(x)
        x = self.maxpool1.forward(x)
        x = self.conv_layer2.forward(x)
        x = self.activation2.forward(x)
        x = self.maxpool2.forward(x)
        x = self.flatten.forward(x)
        x = self.fc_layer.forward(x)
        x = self.softmax.forward(x)
        return x

    def backward(self, x, y):
        # Implement the backward pass to compute gradients
        dLdA = self.softmax.backward(x, y)
        dLdA, dLdW, dLdB = self.fc_layer.backward(dLdA)
        dLdA = self.flatten.backward(dLdA)
        dLdA = self.maxpool2.backward(dLdA)
        dLdA = self.activation2.backward(dLdA)
        dLdA = self.conv_layer2.backward(dLdA)
        dLdA = self.maxpool1.backward(dLdA)
        dLdA = self.activation1.backward(dLdA)
        dLdA = self.conv_layer1.backward(dLdA)


    def update(self, learning_rate):
        # Implement the parameter updates
        self.fc_layer.weights -= (learning_rate * self.fc_layer.grad_weights)
        self.fc_layer.biases -= (learning_rate * self.fc_layer.grad_biases)
        self.conv_layer1.weights -= (learning_rate * self.conv_layer1.grad_weights)
        self.conv_layer2.weights -= (learning_rate * self.conv_layer2.grad_weights)


    def loss_function(self, o):
      e = 1e-15
      o = np.squeeze(o)
      o = np.clip(o, e, 1-e)
      loss = 0
      for i in range(len(o)):
        if i == self.la:
          loss -= np.log(o[i])
        else:
          loss -= np.log(1-o[i])
      return loss

    def train(self, train_data, train_labels, learning_rate):
        x = np.array([train_data])
        y = np.array([train_labels])
        y_pred = self.forward(x).T
        self.backward(y_pred, y)
        self.update(learning_rate)

        a = y_pred.reshape(10,)
        b = train_labels.reshape(10,)
        self.la = np.argmax(b)
        self.Loss_train.append(self.loss_function(a))

    def test(self, test_data, test_labels):
        x = np.array([test_data])
        y = np.array([test_labels])
        y_pred = self.forward(x).T

        a = y_pred.reshape(10,)
        b = test_labels.reshape(10,)
        self.la = np.argmax(b)
        self.Loss_test.append(self.loss_function(a))
        self.confusion_prediction.append(np.argmax(a))
        self.confusion_true.append(self.la)

        if np.argmax(a) == self.la:
          self.top = x.reshape(28,28)
          if len(self.top3[self.la]) < 3:
            self.top3[self.la].append((a[self.la],self.top))
            self.top3[self.la].sort(key=lambda x:x[0])
          else:
            if self.top3[self.la][0][0] < a[self.la]:
              self.top3[self.la][0] = (a[self.la], self.top)

b_size = 8
model = CNNModel()
num_epochs = 10
learning_rate = 0.01

train_dataloader = Dataloader(shuffle = False, batch_size = b_size)
test_dataloader = Dataloader(is_train = False, shuffle=False, batch_size = b_size)

# train & test loss
train_loss = []
test_loss = []
for epoch in range(num_epochs):
  for batch_images, batch_labels in train_dataloader:  # batch_images = [batch_size, 1, 28, 28] / batch_labels = [batch_size, 10]
    rand = np.random.randint(b_size)
    model.train(batch_images[rand], batch_labels[rand], learning_rate)
  train_loss.append(sum(model.Loss_train)/len(model.Loss_train))
  for batch_images, batch_labels in test_dataloader:
    rand = np.random.randint(b_size)
    model.test(batch_images[rand], batch_labels[rand])
  test_loss.append(sum(model.Loss_test)/len(model.Loss_test))

plt.plot(train_loss, label = 'Training Loss', color = 'blue')
plt.plot(test_loss, label = 'Test Loss', color = 'red')
plt.title('Training & Test Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# confusion matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns

model.reset()

for batch_images, batch_labels in test_dataloader:
  for i in range(b_size):
    model.test(batch_images[i], batch_labels[i])

cm = confusion_matrix(model.confusion_true, model.confusion_prediction, normalize = 'true')
sns.heatmap(cm, annot=True, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# top3
for i in range(len(model.top3)):
  for o in range(len(model.top3[i])):
    plt.subplot(1,3,o+1)
    plt.imshow(model.top3[i][o][1], cmap = 'gray')
    plt.title(f"p : {round(model.top3[i][o][0],5)}")
  plt.show()

