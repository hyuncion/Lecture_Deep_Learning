# -*- coding: utf-8 -*-
"""MLP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-1Lb9dlQciBTQSyfa-owBQaBpeIa0MF8
"""

import gzip
import numpy as np
from pathlib import Path
import math
import matplotlib.pyplot as plt

#### DATA LOAD ####


class Dataloader():
    def __init__(self, path='', is_train=True, shuffle=True, batch_size=8):
        path = Path(path)
        imagePath = Path(path/'train-images-idx3-ubyte.gz') if is_train else Path(path/'t10k-images-idx3-ubyte.gz')
        labelPath = Path(path/'train-labels-idx1-ubyte.gz') if is_train else Path(path/'t10k-labels-idx1-ubyte.gz')

        self.batch_size = batch_size
        self.images = self.loadImages(imagePath)
        self.labels = self.loadLabels(labelPath)
        self.index = 0
        self.idx = np.arange(0, self.images.shape[0])
        if shuffle: np.random.shuffle(self.idx) # shuffle images

    def __len__(self):
        n_images, _, _, _ = self.images.shape
        n_images = math.ceil(n_images / self.batch_size)
        return n_images

    def __iter__(self):
        return datasetIterator(self)

    def __getitem__(self, index):
        image = self.images[self.idx[index * self.batch_size:(index + 1) * self.batch_size]]
        label = self.labels[self.idx[index * self.batch_size:(index + 1) * self.batch_size]]
        image = image/255.0
        return image, label

    def loadImages(self, path):
        with gzip.open(path) as f:
            images = np.frombuffer(f.read(), 'B', offset=16)
            images = images.reshape(-1, 1, 28, 28).astype(np.float32)
            return images

    def loadLabels(self, path):
        with gzip.open(path) as f:
            labels = np.frombuffer(f.read(), 'B', offset=8)
            rows = len(labels)
            cols = labels.max() + 1
            one_hot = np.zeros((rows, cols)).astype(np.uint8)
            one_hot[np.arange(rows), labels] = 1
            one_hot = one_hot.astype(np.float64)
            return one_hot

# for enumerate magic python function returns Iterator
class datasetIterator():
    def __init__(self, dataloader):
        self.index = 0
        self.dataloader = dataloader

    def __next__(self):
        if self.index < len(self.dataloader):
            item = self.dataloader[self.index]
            self.index += 1
            return item
        # end of iteration
        raise StopIteration

#### MLP ####

# Base
class Activation():
  def __init__(self, typ):
    self.typ = typ
    types = ['relu', 'linear', 'softmax']
    if not self.typ in types:
      raise ValueError

  def act(self, x):
    if self.typ == 'relu':
      return np.maximum(0, x)

    elif self.typ == 'linear':
      return x

    elif self.typ == 'softmax':
      x = np.clip(x, -500, 500)
      return np.exp(x) / np.exp(x).sum(axis = 0)

  def gradient(self, x):
    o = np.copy(x)
    if self.typ == 'relu':
      o[x>0] = 1
      o[x<0] = 0
      o[x==0] = 0.5
      return o

    elif self.typ == 'linear':
      return 1

  def forward(self, x):
    return self.act(x)

  def backward(self,pre_grad, pre_w):
    return np.dot(pre_w.T, pre_grad)

# for NN
class Node(): # Linear Layer
    def __init__(self, dimension):
      self.dimension = dimension
      self.w = self.initialization(self.dimension)
      self.b = np.zeros((self.dimension[0], 1))
      # bias는 w와는 다르게 하나의 layer 안에서 동일해야 한다.
      self.dw = None
      self.db = None
      self.d_pre_activation = None # this is a problem

    def initialization(self, d):
      stddev = np.sqrt(2.0 / d[1]) # 1.0 = xavier, 2.0 = he
      res = np.random.uniform(-1 * stddev, stddev, d)
      return res

    def forward(self, x):
      return np.dot(self.w, x) + self.b

    def backward(self, prev_act, prev_w, grad_act, prev_act_grad):
      self.d_pre_activation = np.multiply(grad_act,prev_act_grad)
      self.dw = np.dot(self.d_pre_activation, prev_act.T)
      self.db = np.sum(self.d_pre_activation, axis = 1, keepdims = True)


class Neural_Layer(): # Node & Activation
  def __init__(self, dimension, prev_layer, activate_type):

    self.dimension = dimension
    self.node = Node(self.dimension)
    self.output = Activation(activate_type)

    self.temp_node_f = None
    self.temp_act_f = None

    self.temp_node_b = None
    self.temp_act_b = None

  def forward(self, x):
    self.temp_node_f = self.node.forward(x)
    self.temp_act_f = self.output.forward(self.temp_node_f)
    return self.temp_act_f

  def backward(self, prev_act, prev_w, prev_node_grad):
    grad_node = self.output.gradient(self.temp_node_f)
    self.temp_act_b = self.output.backward(prev_node_grad, prev_w)
    self.node.backward(prev_act, prev_w, self.temp_act_b, grad_node)
    self.temp_node_b = self.node.d_pre_activation

# NN
class NN_Model():
  # Input(28*28) / L & R / L & R / L and Softmax / Output(10)
  def __init__(self, h_dimension = (512,128), h_num = 2, learning_rate = 0.001, hidden_activations = ['relu', 'relu']):
    self.h_dimension = h_dimension
    self.h_num = h_num
    self.lr = learning_rate
    self.hidden_activations = hidden_activations

    self.num_labels = 10
    self.input_layer = None
    self.output_layer = None

    self.input_dim = 784

    self.train_x = None
    self.train_y = None
    self.test_x = None
    self.test_y = None
    self.hidden_layers = []

    self.la = -1
    self.initialize()

    self.Loss_train = []
    self.Loss_test = []
    self.confusion_true = []
    self.confusion_prediction = []
    self.top3 = [[],[],[],[],[],[],[],[],[],[]]
    self.top = None

  def initialize(self):
    input_size = self.input_dim
    prev_layer = None

    for i in range(self.h_num):
      hidden_layer = Neural_Layer((self.h_dimension[i],input_size), prev_layer, self.hidden_activations[i])
      self.hidden_layers.append(hidden_layer)
      input_size = self.h_dimension[i]

    self.output_layer = Neural_Layer((self.num_labels, self.h_dimension[-1]), self.hidden_layers[-1], 'softmax')

  def data_load(self, data, label, mode = 'train'):
    # data : (28, 28) / label : (10,)
      if mode == 'train':
        data = data.reshape(-1, 1) # flatten (784,1)
        self.train_x = data
        self.train_y = label
        for i in range(len(self.train_y)):
          if self.train_y[i] == 1:
            self.la = i
            break

      elif mode == 'test':
        self.top = data
        data = data.reshape(-1,1)
        self.test_x = data
        self.test_y = label
        for i in range(len(self.test_y)):
          if self.test_y[i] == 1:
            self.la = i
            self.confusion_true.append(self.la)
            break

  def reset(self):
    self.confusion_true = []
    self.confusion_prediction = []
    self.top3 = [[],[],[],[],[],[],[],[],[],[]]

  def forward(self, x):
    temp_x = x
    for i in range(self.h_num):
      temp_x = self.hidden_layers[i].forward(temp_x)
    o = self.output_layer.forward(temp_x)
    return o


  def backward(self, train_x, train_y):
    # true
    a = np.eye(self.num_labels)[self.la].reshape(-1,1)
    # error
    self.output_layer.temp_act_b = -1 * (a / self.output_layer.temp_act_f)
    self.output_layer.temp_node_b = -1 * (a - self.output_layer.temp_act_f)

    self.output_layer.node.dw = np.dot(self.output_layer.temp_node_b, self.hidden_layers[-1].temp_act_f.T)
    self.output_layer.node.db = np.sum(self.output_layer.temp_node_b, axis = 1, keepdims = True)

    prev_layer_act_grad = self.output_layer.temp_node_b
    next_layer_w = self.output_layer.node.w

    for i in range(self.h_num -1, -1, -1):
      if i != 0:
        prev_act_f = self.hidden_layers[i-1].temp_act_f
      else:
        prev_act_f = train_x

      self.hidden_layers[i].backward(prev_act_f, next_layer_w, prev_layer_act_grad)
      prev_layer_act_grad = self.hidden_layers[i].temp_node_b
      next_layer_w = self.hidden_layers[i].node.w

  def update(self):
    for i in range(0, self.h_num):
      self.hidden_layers[i].node.w = self.hidden_layers[i].node.w - self.lr * self.hidden_layers[i].node.dw
      self.hidden_layers[i].node.b = self.hidden_layers[i].node.b - self.lr * self.hidden_layers[i].node.db

    self.output_layer.node.w = self.output_layer.node.w - self.lr * self.output_layer.node.dw
    self.output_layer.node.b = self.output_layer.node.b - self.lr * self.output_layer.node.db

  def loss_function(self, o):
    e = 1e-15
    o = np.squeeze(o)

    o = np.clip(o, e, 1-e)
    loss = 0
    for i in range(len(o)):
      if i == self.la:
        loss -= np.log(o[i])
      else:
        loss -= np.log(1-o[i])
    return loss

  def accuracy(self, o):
    p = np.argmax(o)
    if p == self.la:
      self.count+=1
    self.total+=1
    return (self.count/self.total) * 100

  def train(self):
    o = self.forward(self.train_x)
    self.backward(self.train_x, self.train_y)
    self.update()
    self.Loss_train.append(self.loss_function(o)) # training loss

  def test(self):
    o = self.forward(self.test_x)
    self.Loss_test.append(self.loss_function(o))
    p = np.squeeze(o)
    self.confusion_prediction.append(np.argmax(p))

    if np.argmax(p) == self.la:
      if len(self.top3[self.la]) < 3:
        self.top3[self.la].append((o[self.la], self.top))
        self.top3[self.la].sort(key=lambda x:x[0])
      else:
        if self.top3[self.la][0][0] < p[self.la]:
          self.top3[self.la][0] = (o[self.la], self.top)
          self.top3[self.la].sort(key=lambda x:x[0])

# DNN
# initialize
b_size = 8
# __init__(self, h_dimension = (512,128), h_num = 2, batch_size = 8, learning_rate = 0.1, hidden_activations = ['linear', 'relu']):
model = NN_Model()
# data load
train_dataloader = Dataloader(shuffle = False, batch_size = b_size)
test_dataloader = Dataloader(is_train = False, shuffle = False, batch_size = b_size)
# train
num_epochs = 100
L = []
TL = []
for epoch in range(num_epochs):
  for batch_images, batch_labels in train_dataloader:   # batch_images = [batch_size, 1, 28, 28] / batch_labels = [batch_size, 10]
    # sgd를 통해 random하게 하나씩만 가져오자.
    rand = np.random.randint(b_size)
    model.data_load(batch_images[rand][0], batch_labels[rand], mode = 'train')
    model.train()
  L.append(sum(model.Loss_train)/len(model.Loss_train))
  for batch_images, batch_labels in test_dataloader:
    rand = np.random.randint(b_size)
    model.data_load(batch_images[rand][0], batch_labels[rand], mode = 'test')
    model.test()
  TL.append(sum(model.Loss_test)/len(model.Loss_test))

# Training & Test Loss
plt.plot(L, label='Training Loss', color='blue')
plt.plot(TL, label='Test Loss', color = 'red')
plt.title('Training & Test Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

model.reset()

# confusion matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns
# test 나중에 training의 epoch for loop 안에 넣어야 한다. for loss graph.
for batch_images, batch_labels in test_dataloader:    # batch_images = [batch_size, 1, 28, 28] / batch_labels = [batch_size, 10]
  for i in range(b_size):
    model.data_load(batch_images[i][0], batch_labels[i], mode = 'test')
    model.test()
cm = confusion_matrix(model.confusion_true, model.confusion_prediction, normalize = 'true')
sns.heatmap(cm, annot=True, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# top3 images
for i in range(len(model.top3)):
  for o in range(len(model.top3[i])):
    plt.subplot(1,3,o+1)
    plt.imshow(model.top3[i][o][1], cmap = 'gray')
    plt.title(f"p : {round(model.top3[i][o][0][0],5)}")
  plt.show()

